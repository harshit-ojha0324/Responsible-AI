{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf67c18a",
   "metadata": {},
   "source": [
    "# RESP AI Findings Demo\n",
    "This notebook walks through the evaluation results for outcome, process, and structured response styles on a GSM8K slice.\n",
    "\n",
    "**Highlights**\n",
    "- Process-style reasoning hit 1.00 accuracy vs. 0.86 for outcome/structured; McNemar p=0.617 shows differences are not statistically significant at this sample size.\n",
    "- Structured outputs were most auditable (clarity/verification/coherence ? 3.86) while process traces were the most faithful to the underlying reasoning (2.0 mean faithfulness).\n",
    "- Accuracy is most correlated with faithfulness (r?0.95) and moderately with step correctness (r?0.69).\n",
    "- The notebook reloads metrics from `results/`, recreates a few plots, and embeds the saved figures for quick review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "project_root = Path.cwd()\n",
    "if project_root.name == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "results_dir = project_root / \"results\"\n",
    "print(f\"Using results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir / \"accuracy_metrics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    accuracy_metrics = json.load(f)\n",
    "with open(results_dir / \"interpretability_metrics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    interpretability_metrics = json.load(f)\n",
    "with open(results_dir / \"statistical_analysis.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    statistical_analysis = json.load(f)\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_metrics[\"accuracy_comparison\"][\"comparison_table\"])\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0178728",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(4, 3))\n",
    "sns.barplot(data=accuracy_df, x=\"Condition\", y=\"Accuracy\", palette=\"Blues_d\", ax=ax)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_title(\"Accuracy by condition\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2, p.get_height() + 0.01), ha=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "stat = accuracy_metrics[\"accuracy_comparison\"]\n",
    "print(\"McNemar outcome vs process p-value:\", round(accuracy_metrics.get(\"mcnemar_outcome_vs_process_pvalue\", float('nan')), 3))\n",
    "print(\"Effect size (outcome vs process):\", round(stat.get(\"effect_size_outcome_vs_process\", float('nan')), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for condition, metrics in interpretability_metrics[\"summary\"].items():\n",
    "    row = {\"condition\": condition}\n",
    "    for key in [\"step_correctness\", \"faithfulness\", \"clarity\", \"verification_effort\", \"coherence\"]:\n",
    "        row[key] = metrics[key][\"mean\"]\n",
    "    rows.append(row)\n",
    "interpret_df = pd.DataFrame(rows).set_index(\"condition\").round(3)\n",
    "interpret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = interpret_df.reset_index().melt(id_vars=\"condition\", var_name=\"metric\", value_name=\"score\")\n",
    "order = [\"process\", \"structured\"]\n",
    "metrics_order = [\"step_correctness\", \"faithfulness\", \"clarity\", \"verification_effort\", \"coherence\"]\n",
    "_, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.barplot(data=plot_df, x=\"metric\", y=\"score\", hue=\"condition\", order=metrics_order, palette=\"Set2\", ax=ax)\n",
    "ax.set_title(\"Interpretability metrics (means)\")\n",
    "ax.set_ylim(0, 4.2)\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_df = pd.DataFrame(statistical_analysis[\"correlations\"][\"pearson\"]).astype(float)\n",
    "_, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(pearson_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title(\"Pearson correlations across metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_dir = results_dir / \"figures\"\n",
    "figure_names = [\n",
    "    \"01_accuracy_comparison.png\",\n",
    "    \"03_interpretability_metrics.png\",\n",
    "    \"05_summary_comparison.png\",\n",
    "    \"06_correlation_heatmap.png\",\n",
    "    \"07_faithfulness_vs_accuracy.png\",\n",
    "]\n",
    "for name in figure_names:\n",
    "    path = figure_dir / name\n",
    "    if path.exists():\n",
    "        display(Image(filename=path))\n",
    "    else:\n",
    "        print(f\"Missing figure: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb42021",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Swap in a larger evaluation set and rerun the pipeline to firm up the significance tests.\n",
    "- Add error analysis cells to inspect mispredicted items and their rationales.\n",
    "- Wire this notebook into the report workflow (e.g., papermill) so it refreshes alongside new results."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
